---
title: "chapter_2"
output: html_notebook
---

Link: https://supervised-ml-course.netlify.app/chapter2  
github: https://github.com/juliasilge/supervised-ML-case-studies-course  
stackoverflow source: https://insights.stackoverflow.com/survey  
This nb contains code from exercises of Chapter 2.  
  
This chapter uses a dataset from Stack Overflow Developer Survey. We will start with some EDA then we will train classification models.  
The specific question we will address is what makes a developer more likely to work remotely. There are characteristics of developers such as size of company, what work they do, experience or where in the world they live that affect how likely they are to be working remotely.  

Load the data from github: 
```{r}
stack_overflow <- read.csv("https://raw.githubusercontent.com/juliasilge/supervised-ML-case-studies-course/master/data/stack_overflow.csv")
```

Load packages
```{r}
library(tidyverse)
```

Immediately we will see that the proportion of developers who are remote and who work in the office is not balanced.
```{r}
stack_overflow %>%
  count(remote)
```
This type of class imbalance can have significant negative impact on model performance, we we need to do some preprocessing of the data before we model it.  
  
# Explore the Stack Overflow survey

```{r}
glimpse(stack_overflow)

# Count for remote
stack_overflow %>%
  count(remote, sort = TRUE)

# Count for country
stack_overflow %>%
  count(country, sort = TRUE)
```

Make a boxplot with remote status on the x-axis and professional experience on the y-axis.
```{r}
stack_overflow %>%
  ggplot(aes(x = remote, y = years_coded_job)) +
  geom_boxplot() +
  #geom_jitter(alpha = .25, color = "blue") +
  labs(x = NULL,
       y = "Years of professional coding exp")
```
We see that the more experience devs have, the more likely they are to work remotely.

# Training and testing data

Before we deal with imbalances in the remote/not remote classes, first we split the data into training and testing sets. We do this to reduce overfitting and to obtain a more accurate estimate for how our model will perform on new data.  
  
Instructions:  
- Create split of 80/20 sections and about evenly divides the sections between different classes of `remote`. Using `initial_split()`, stratify the split by remote status.  
- Use `training()`to assign the 80% partition to `stack_train`and use `testing()` to assign the 20% to `stack_test`.  
```{r}
library(tidymodels)

# factor remote
stack_overflow$remote <- factor(stack_overflow$remote)
str(stack_overflow)

# create stack_select dataset
stack_select <- stack_overflow %>%
  select(-respondent)

# split data into training and testing sets
set.seed(1234)
stack_split <- stack_select %>%
  initial_split(prop = 0.8,
                strata = remote)

stack_train <- training(stack_split)
stack_test  <- testing(stack_split)
```

# Dealing with imbalanced data

Class imbalance  
  
- is a common problem  
- often negatively affects the performance of your model  

```{r}
stack_train %>%
  count(remote)
```
There are about 10 times more non-remote workers than remote workers. What can happen is that a machine learning model will always predict the majority class or otherwise exhibit poor performance on the metrics that we care about.  
  
There are several ways to deal with this problem, they vary from simple to more complex ways and we will start with a simple option.  
  
Down sampling  
  
- Remove some of the majority class so it has less effect on the predictive model  
- Randomly remove examples from the majority class until it is the same size as the minority class.  
  
Yes, we throw away a large percentage of our data, because it can help us produce a useful model that can recognize both classes instead of just one.  
  
We will remove some non-remote devs. We can preprocess your data using recipes. The recipe shown in this slide only has one preprocessing step (downsampling, that comes from an extra add-on package called themis), but you can implement many steps on one dataset during preprocessing. There are an enormous number of different kinds of preprocessing you can do, from creating indicator variables to implementing principal component analysis to extracting date features and more.  
  
```{r}
library(themis)

stack_recipe <- recipe(remote ~ ., data = stack_train) %>%
  step_downsample(remote)
```
Implementing downsampling

```{r}
stack_prep <- prep(stack_recipe)

bake(stack_prep, new_data = NULL)
```


When you `prep()` a recipe, you estimate the required parameters from a data set for the preprocessing steps in that recipe (as an example, think about finding the mean and standard deviation if you are centering and scaling).

When you `bake()` a prepped recipe with new_data = NULL, you get the preprocessed data back out.

You don't typically need to `prep()` and `bake()` recipes when you use tidymodels, but they are helpful functions to have in your toolkit for confirming that recipes are doing what you expect.

# Preprocess with receipe  

There are multiple ways to deal with class imbalances. Just like in above example we will implement downsampling usig the `step_downsample()` function from `themis` package.  
  
Instructions:  
- Use `recipe()` to preprocess training data
- Downsample the data with respect to the remote status of the devs.  
```{r}
stack_recipe <- recipe(remote ~ ., data = stack_train) %>%
  step_downsample(remote)

stack_recipe
```
# Downsampling  

Once your recipe is defined, you can estimate the parameters required to actually preprocess the data, and then extract the processed data. This typically isn’t necessary is you use a `workflow()` for modeling, but it can be helpful to diagnose problems or explore your preprocessing results.

Instrucitons
- First `prep()` the recipe  
- Then `bake()` the prepped recipe with `new_data = NULL` to see the processed training data.  
```{r}
stack_prep <- prep(stack_recipe)
stack_down <- bake(stack_prep, new_data = NULL)

stack_down %>%
  count(remote)
```
When you `bake()` the prepped recipe stack_prep with new_data = NULL, you extract the processed (i.e. balanced) training data.  
  
Understanding downsampling.  
Downsampling removes from the majority class until the class distributions are equal, so there are the same number of remote and non-remote developers after downsampling. We downsampled the training data. This will help us train a model that performs better.
    
Now you have a data set with balanced classes, ready for machine learning!  
  
# Predicting remote status  
  
Build supervised machine learning models to predict which devs work remotely and which do not.  
  
# Train models  
  
We will specify our ml models using `parsnip` and use workflows for convenience. https://workflows.tidymodels.org/  
  
Instruction:  
- Specify logistic regression model using logistic_reg()  
- Build a `workflow()` to hold the modeling components  
- Add the model specification to the `workflow()` before fitting.  

```{r}
# downsample as we did above
stack_recipe <- recipe(remote ~ ., data = stack_train) %>%
  step_downsample(remote)

# build a logistic regression model
glm_spec <- logistic_reg() %>%
  set_engine("glm")

# start a workflow (only recipe)
stack_wf <- workflow() %>%
  add_recipe(stack_recipe)

# add the model and fit the workflow
stack_glm <- stack_wf %>%
  add_model(glm_spec) %>%
  fit(data = stack_train)

# print the fitted model
stack_glm
```
  
Instructions:  
  
Build a decision tree model with downsampling.  
  
- Specify a decition tree regression model using `decision_tree()`  
- Add you recipe `stack_recipe` to you `workflow()` 
- Fit you workflow, after you have added your model to it.  
  
```{r}
# downsample as we did above
stack_recipe <- recipe(remote ~ ., data = stack_train) %>%
  step_downsample(remote)

# build a decision tree model
tree_spec <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")

# start a workflow (recipe only)
stack_wf <- workflow() %>%
  add_recipe(stack_recipe)

# add the model and fit the workflow
stack_tree <- stack_wf %>%
  add_model(tree_spec) %>%
  fit(data = stack_train)

# print the fitted model
stack_tree
```
We have trained two ml models.  
  
# Confusing matrix  
  
A confusion matrix describes how well a classification model (like the ones you just trained!) performs. A confusion matrix tabulates how many examples in each class were correctly classified by a model. In your case, it will show you how many remote developers were classified as remote and how many non-remote developers were classified as non-remote; the confusion matrix also shows you how many were classified into the *wrong* categories.

Here you will use the `conf_mat()` function from yardstick to evaluate the performance of the two models you trained, `stack_glm` and `stack_tree`. The models available in your environment were trained on the training data.  
  
-Instructions  

Print the confusion matrix for the `stack_glm` model on the `stack_test` data. If we wanted to compare more than two modeling options, we should definitely create some resampled data sets like we did in the first case study. This case study is already getting long, so let’s stick with the testing data.

Note that the first argument to `conf_mat()` is `truth` and the second is `estimate`.
```{r}
results <- stack_test %>%
  bind_cols(predict(stack_glm, stack_test) %>%
              rename(.pred_glm = .pred_class))

# Confusion matrix for logistic regression model
results %>%
  conf_mat(truth = remote, estimate = .pred_glm)
```
  
- Instructions  

Print the confusion matrix for the `stack_tree` model on the `stack_test` data.

```{r}
results <- stack_test %>%
    bind_cols(predict(stack_tree, stack_test) %>%
                  rename(.pred_tree = .pred_class))

# Confusion matrix for decision tree model
results %>%
    conf_mat(truth = remote, estimate = .pred_tree)
```
The confusion matrix is used to evaluate the model performance, so we should use the testing set.  
  
# Classification model metrics  
  
The `conf_mat()` function is helpful but often you also want to store specific performance estimates for later, perhaps in a dataframe-friendly form. The yardstick package is built to handle such needs. For this kind of classification model, you might look at the positive or negative predictive value or perhaps overall accuracy.

The models available in your environment, `stack_glm` and `stack_tree` were trained on the training data.

Instructions

- Predict values for logistic regression (`stack_glm`) and decision tree (`stack_tree`).
- Calculate both accuracy and positive predictive value for these two models.  
```{r}
results <- stack_test %>%
  bind_cols(predict(stack_glm, stack_test) %>%
              rename(.pred_glm = .pred_class)) %>%
  bind_cols(predict(stack_tree, stack_test) %>%
              rename(.pred_tree = .pred_class))

# Calculate accuracy
accuracy(results, truth = remote, estimate = .pred_glm)
accuracy(results, truth = remote, estimate = .pred_tree)

# Caclulate positive predict value
ppv(results, truth = remote, estimate = .pred_glm)
ppv(results, truth = remote, estimate = .pred_tree)
```
Conclusion:  
In terms of overall accuracy and positive predictive value, the decision tree model outperforms the logistic regression model. We can predict the remote status of a developer more accurately with a decision tree model.

